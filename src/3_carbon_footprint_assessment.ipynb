{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d69ab20796085f62",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Carbon Footprint Assessment:\n",
    "Develop a metric for evaluating building carbon footprints, accounting for energy and water consumption, size, and other factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "initial_id",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-08T02:28:00.816750Z",
     "start_time": "2023-10-08T02:28:00.801152Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "679630cc2668661e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-08T02:28:00.818059Z",
     "start_time": "2023-10-08T02:28:00.804797Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the schema from the JSON file\n",
    "with open('./data/schema_cleaned2.json') as schema_file:\n",
    "    schema = json.load(schema_file)\n",
    "\n",
    "# Convert schema to the format required by pandas\n",
    "# Note: In this case, we're assuming all the dtypes are compatible with pandas dtypes.\n",
    "# If there are any discrepancies, you may need to manually adjust the dtypes.\n",
    "pandas_schema = {key: value for key, value in schema.items()}\n",
    "feature_list = [key for key, value in schema.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea6ab7f7d2f954d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Data Preparation\n",
    "Since not all our data is numerical, we'll need to do some one-hot encoding.\n",
    "Given all the categorical columns, we're going to use pandas library for one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "fd682c454445f26e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-08T02:28:00.902078Z",
     "start_time": "2023-10-08T02:28:00.814238Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now read the cleaned CSV file using the schema\n",
    "df_cleaned = pd.read_csv('./data/data_cleaned2.csv', dtype=pandas_schema)\n",
    "\n",
    "# Select the categorical columns that need one-hot encoding\n",
    "categorical_columns = [key for key, value in schema.items() if value == 'object']\n",
    "numerical_columns = [key for key, value in schema.items() if value == 'float64']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc102983c7d2d184",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Feature Selection & Scaling\n",
    "Choosing the relevant features for clustering the different types of buildings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "e1463ab6046ce462",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-08T02:28:00.909978Z",
     "start_time": "2023-10-08T02:28:00.906055Z"
    }
   },
   "outputs": [],
   "source": [
    "# Select the 'year_built' and 'net_emissions_metric_tons' features\n",
    "selected_features = df_cleaned[numerical_columns]\n",
    "\n",
    "# Scale our data for a mean of 0\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "outputs": [],
   "source": [
    "# Perform dimensionality reduction using PCA\n",
    "num_components = 5\n",
    "pca = PCA(n_components=num_components)  # Adjust the number of components as needed\n",
    "reduced_features = pca.fit_transform(selected_features)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-08T02:28:02.083759Z",
     "start_time": "2023-10-08T02:28:00.915446Z"
    }
   },
   "id": "367735e583d5de54"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Fit the PCA model to your data\n",
    "pca.fit(selected_features)\n",
    "\n",
    "# Get the transformed data (principal components)\n",
    "components = pca.transform(selected_features)\n",
    "\n",
    "# Calculate the explained variance for each component\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "# Plot the explained variance to decide on the number of components to keep\n",
    "plt.bar(range(1, num_components + 1), explained_variance)\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-10-08T02:28:02.085442Z"
    }
   },
   "id": "a855b796e54cc836"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Now, you can examine which original features contribute most to the first few components\n",
    "feature_contributions = abs(pca.components_[num_components - 1])\n",
    "sorted_features = sorted(zip(selected_features.columns, feature_contributions), key=lambda x: -x[1])\n",
    "\n",
    "# Print the most important features for the chosen component\n",
    "print(f\"Most important features for Principal Component:\")\n",
    "for feature, contribution in sorted_features:\n",
    "    print(f\"{feature}: {contribution:.2f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "eac0923d2c828584"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## K-Means Clustering\n",
    "After dimensionality reduction, we proceed with K-Means clustering on the reduced feature set. The goal is to cluster data points in this lower-dimensional space into K clusters."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f8f8508ef670e96b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Choose the number of clusters (K)\n",
    "K = 5  # Replace with the desired number of clusters\n",
    "\n",
    "# Create a K-Means model\n",
    "kmeans_model = KMeans(n_clusters=K, random_state=42)\n",
    "\n",
    "# Fit the K-Means model to the selected features\n",
    "kmeans_model.fit(selected_features)\n",
    "\n",
    "# Get cluster assignments for each data point in the dataset\n",
    "cluster_assignments = kmeans_model.predict(selected_features)\n",
    "\n",
    "# Create a scatter plot to visualize the clusters\n",
    "plt.scatter(selected_features['year_built'], selected_features['net_emissions_metric_tons'], c=cluster_assignments, cmap='rainbow')\n",
    "plt.xlabel('Year Built')\n",
    "plt.ylabel('Net Emissions (metric tons)')\n",
    "plt.title('K-Means Clustering')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "815eb3350f92e226"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
